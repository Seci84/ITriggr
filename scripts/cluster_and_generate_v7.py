import os
import re
import json
import time
import traceback
from collections import defaultdict
from firebase_admin import firestore
from common import init_db, log_event, sim_prefix
from openai.types.chat.completion_create_params import ResponseFormat
from openai import OpenAI
from google.cloud.firestore_v1.base_query import FieldFilter
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# --- OpenAI ì‚¬ìš© ì—¬ë¶€ ---
client = None  # âœ… í•­ìƒ ë¯¸ë¦¬ ì •ì˜
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
USE_OPENAI = os.getenv("USE_OPENAI", "False").lower() == "true"

if OPENAI_API_KEY and USE_OPENAI:
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)  # proxies ì¸ì ì œê±°
        print("âœ… OpenAI client initialized successfully")
    except Exception as e:
        print(f"âŒ OpenAI client init failed: {e}")
        print("Full stack trace:")
        traceback.print_exc()
        USE_OPENAI = False
else:
    print(f"USE_OPENAI = {USE_OPENAI}, OPENAI_API_KEY is {'set' if OPENAI_API_KEY else 'not set'}")

# --- LLM í”„ë¡¬í”„íŠ¸ (JSON only) ---
# âœ… ìŠ¤í‚¤ë§ˆë¥¼ talks(êµ¬ì–´ì²´ ë¬¸ë‹¨) ì¤‘ì‹¬ìœ¼ë¡œ ë³€ê²½
PROMPT = """You are a news rewrite assistant. Return ONLY a single JSON object with no code fences, no explanations, and no comments.

Required JSON shape (all fields are MANDATORY and must match exactly):
{
  "title": "string",
  "summary": "string",
  "bullets": ["string", "string", "string"],
  "facts": [{"text":"string","evidence_url":"string"}],
  "talks": {
    "general": "string",
    "entrepreneur": "string",
    "politician": "string",
    "investor": "string"
  }
}

Rules:
- Strictly adhere to the exact JSON shape above. Any deviation (e.g., comments, code blocks, explanations) will result in rejection.
- Detect the category (politics, economy, society, tech, military, etc.) from the content and tailor the analysis.
- Use available sources (one or more). Cite at least 1 item in "facts" with evidence_url chosen from the given Sources list. Be specific with entities (companies, products, laws).
- Analyze the full content of each source URL to inform title, summary, bullets, facts, and talks.
- Tone: cautious and factual. No guarantees or advice. Use phrases like "consider", "possible idea".
- Language: If most sources are Korean, write Korean; otherwise English.
- "talks" must be a conversational paragraph (2â€“4 sentences each, in the selected language) that naturally weaves together an action suggestion, the underlying assumption/context, a risk to watch, and a practical alternative. Avoid bullet-like structure; write as smooth natural language.
- Avoid financial or policy advice; keep it interpretive and neutral.

Sources:
{sources}
"""

def safe_parse_json(content: str):
    """LLM ì‘ë‹µì—ì„œ JSONë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ."""
    try:
        return json.loads(content)
    except Exception:
        pass
    content2 = re.sub(r"^```(?:json)?\s*|\s*```$", "", content.strip(), flags=re.I | re.M)
    try:
        return json.loads(content2)
    except Exception:
        pass
    m = re.search(r"\{.*\}", content, flags=re.S)
    if m:
        return json.loads(m.group(0))
    raise ValueError(f"JSON parse failed. head={content[:120]!r}")

def fetch_content(url):
    """URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        # ê°„ë‹¨í•œ ë³¸ë¬¸ ì¶”ì¶œ (ì‚¬ì´íŠ¸ë³„ë¡œ ì¡°ì • í•„ìš”)
        paragraphs = soup.find_all('p')
        content = ' '.join(p.get_text() for p in paragraphs if p.get_text().strip())
        return content[:1000]  # í† í° ì œí•œìœ¼ë¡œ 1000ì ì œí•œ
    except Exception as e:
        print(f"Failed to fetch content from {url}: {e}")
        return "Content unavailable"

def load_recent_raw_groups(db, window_sec=6 * 60 * 60, prefix_bits=16,
                           exclude_domains=("nytimes.com", "nyti.ms")):
    """
    ìµœê·¼ ì›ë¬¸ ê¸°ì‚¬ë“¤ì„ simhash prefixë¡œ í´ëŸ¬ìŠ¤í„°ë§.
    NYT ë„ë©”ì¸(nytimes.com, nyti.ms)ì€ ì œì™¸.
    """
    def _is_excluded(url: str) -> bool:
        try:
            host = urlparse(url or "").netloc.lower()
            if not host:
                return False
            # example: sub.domain.nytimes.com ë„ í•¨ê»˜ ì œì™¸
            return any(host == d or host.endswith("." + d) for d in exclude_domains)
        except Exception:
            return False

    now = int(time.time())
    since = now - window_sec
    q = db.collection("raw_articles").where(filter=FieldFilter("published_at", ">=", since))
    groups = defaultdict(list)

    total, skipped = 0, 0
    for d in q.stream():
        it = d.to_dict() or {}
        total += 1
        url = it.get("url", "") or ""
        if _is_excluded(url):
            skipped += 1
            continue

        k = sim_prefix(it.get("simhash", ""), prefix_bits=prefix_bits)
        groups[k].append((d.id, it))

    print(f"Loaded {len(groups)} clusters from raw_articles (total_docs={total}, skipped_nyt={skipped})")
    return groups

def already_generated(db, cluster_key: str) -> bool:
    """
    íŠ¹ì • cluster_keyì— ëŒ€í•´ ì´ë¯¸ ìƒì„±ëœ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸.
    (generated_articles_v3 ê¸°ì¤€)
    """
    try:
        snap = (db.collection("generated_articles_v3")
                  .where(filter=FieldFilter("cluster_key", "==", cluster_key))
                  .limit(1)
                  .get())
        return len(snap) > 0
    except Exception as e:
        print(f"already_generated check failed: {e}")
        return False

# âœ… í…œí”Œë¦¿(LLM ë¹„í™œì„±/ì‹¤íŒ¨ ì‹œ)ë„ talksë§Œ ìƒì„±í•˜ë„ë¡ ìˆ˜ì •
def make_payload_from_sources(items):
    """OPENAI ë¹„í™œì„±/ì‹¤íŒ¨ ì‹œ UIê°€ ë°”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” í…œí”Œë¦¿ í˜ì´ë¡œë“œ."""
    n = len(items)
    title = f"[Auto] {n} source{'s' if n > 1 else ''} on same event"
    summary = (
        "Multiple outlets reported a similar event. (Template summary: LLM disabled)"
        if n > 1
        else "A single source reported this event. (Template summary: LLM disabled)"
    )
    bullets = ["Key point 1", "Key point 2", "Key point 3"]
    first = items[0][1] if items else {}
    facts = [{"text": first.get("title", ""), "evidence_url": first.get("url", "")}]

    talks = {
        "general": "ì´ë²ˆ ì´ìŠˆëŠ” ì¼ìƒê³¼ë„ ë§ë‹¿ì•„ ìˆì–´ìš”. ë‹¨ì •í•˜ê¸°ë³´ë‹¤ëŠ” ì£¼ë³€ ì˜ê²¬ì„ ë“¤ì–´ë³´ë©° ìƒí™©ì„ ì²œì²œíˆ ì •ë¦¬í•´ ë³´ì„¸ìš”. ê³¼ì—´ëœ ì£¼ì¥ì—” ê±°ë¦¬ë¥¼ ë‘ê³ , ë„ì›€ì´ ë˜ëŠ” ì‘ì€ ì‹¤ì²œë¶€í„° ì‹œì‘í•˜ë©´ ì¢‹ì•„ìš”.",
        "entrepreneur": "ì‹œì¥ ë°˜ì‘ì´ ì¶œë ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ê³¼ê°í•œ ë©”ì‹œì§€ë³´ë‹¤ ê³ ê° ì¸í„°ë·°ì™€ ì‘ì€ ì‹¤í—˜ìœ¼ë¡œ ê²€ì¦í•´ìš”. ë¦¬ìŠ¤í¬ ë…¸ì¶œì€ ìµœì†Œí™”í•˜ê³ , ëŒ€ì•ˆ ì±„ë„ì´ë‚˜ íŒŒì¼ëŸ¿ìœ¼ë¡œ í•™ìŠµ ì†ë„ë¥¼ ë†’ì—¬ë³´ì£ .",
        "politician": "ì‚¬ì‹¤ê´€ê³„ë¥¼ ìš°ì„  í™•ì¸í•˜ê³  ì´í•´ê´€ê³„ì ì˜ê²¬ì„ í­ë„“ê²Œ ìˆ˜ë ´í•´ ë³´ì„¸ìš”. ì •ìŸìœ¼ë¡œ ë²ˆì§ˆ ì—¬ì§€ê°€ ìˆë‹¤ë©´ ë‹¨ê³„ì  ê¶Œê³ ì•ˆë¶€í„° ì œì‹œí•˜ëŠ” í¸ì´ ì•ˆì „í•©ë‹ˆë‹¤.",
        "investor": "í—¤ë“œë¼ì¸ë³´ë‹¤ í€ë”ë©˜í„¸ê³¼ í˜„ê¸ˆíë¦„ì„ ë¨¼ì € ì‚´í´ë´ìš”. ë³€ë™ì„±ì€ ë¶„ì‚°ê³¼ í¬ì§€ì…˜ í¬ê¸° ì¡°ì ˆë¡œ ê´€ë¦¬í•˜ê³ , ì •ë³´ê°€ ë” ìŒ“ì¼ ë•Œê¹Œì§€ëŠ” ê´€ë§ë„ ì„ íƒì§€ì˜ˆìš”."
    }

    return {
        "title": title,
        "summary": summary,
        "bullets": bullets,
        "facts": facts,
        "talks": talks
    }

def run_once():
    db = init_db()
    groups = load_recent_raw_groups(db)
    created = 0

    for cluster_key, items in groups.items():
        if len(items) < 1:
            continue
        if already_generated(db, cluster_key):
            print(f"Skipping cluster {cluster_key}: already generated")
            continue

        src_lines = []
        ts_min, ts_max = 10 ** 12, 0
        for _id, it in items:
            url = it.get("url", "")
            title = it.get("title", "")
            content = fetch_content(url)  # URLì—ì„œ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            src_lines.append(f"- {title} | {url} | {content}")
            ts = int(it.get("published_at", 0) or 0)
            ts_min, ts_max = min(ts_min, ts), max(ts_max, ts)

        payload = None  # ì´ˆê¸°í™”
        token_usage = {"prompt": 0, "completion": 0}
        latency_ms = 0
        model_used = "template"

        if USE_OPENAI and len(src_lines) >= 1:
            try:
                t0 = time.time()
                prompt = PROMPT.format(sources="\n".join(src_lines))

                print(f"Sending OpenAI request for cluster {cluster_key} with {len(src_lines)} sources")
                resp = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.2,
                    response_format={"type": "json_object"},
                )
                latency_ms = int((time.time() - t0) * 1000)

                try:
                    token_usage["prompt"] = getattr(resp.usage, "prompt_tokens", 0)
                    token_usage["completion"] = getattr(resp.usage, "completion_tokens", 0)
                except Exception:
                    pass

                content = getattr(resp.choices[0].message, "content", None)
                if content is None and isinstance(resp.choices[0].message, dict):
                    content = resp.choices[0].message.get("content", "")

                print("ğŸ” LLM RESPONSE START")
                print(content)
                print("ğŸ” LLM RESPONSE END")

                payload = safe_parse_json(content)
                model_used = "gpt-4o-mini"

            except Exception as e:
                print(f"OpenAI error for cluster {cluster_key}: {repr(e)}")
                print("Trace:\n", traceback.format_exc())
                log_event(db, "openai_error", {
                    "msg": str(e),
                    "raw_content": content if 'content' in locals() else "N/A",
                    "cluster_key": cluster_key
                })
                # GPT ì‹¤íŒ¨ ì‹œ í…œí”Œë¦¿ ì‚¬ìš©
                payload = make_payload_from_sources(items)

        else:
            # OpenAI ë¹„í™œì„±í™” ì‹œ í…œí”Œë¦¿ ì‚¬ìš©
            payload = make_payload_from_sources(items)

        if payload is None:
            payload = make_payload_from_sources(items)  # ì•ˆì „ë§

        # Firestore ë¬¸ì„œ êµ¬ì„± (talks ì¤‘ì‹¬)
        doc = {
            "cluster_key": cluster_key,
            "title": payload.get("title", ""),
            "summary": payload.get("summary", ""),
            "bullets": payload.get("bullets", []),
            "facts": payload.get("facts", []),
            # âœ… ìƒˆ êµ¬ì¡°: êµ¬ì–´ì²´ ë¬¸ë‹¨
            "talks": payload.get("talks", {
                "general": "", "entrepreneur": "", "politician": "", "investor": ""
            }),
            # ì°¸ê³ /ì¶”ì ìš© ë©”íƒ€
            "evidence_urls": [line.split("|")[1].strip() for line in src_lines if "|" in line],
            "raw_refs": [x[0] for x in items],
            "published_window": {"start": ts_min, "end": ts_max},
            "model": model_used,
            "token_usage": token_usage,
            "latency_ms": latency_ms,
            "schema_version": "talks_v1",   # âœ… ìŠ¤í‚¤ë§ˆ ì‹ë³„ìš©
            "created_at": firestore.SERVER_TIMESTAMP,
        }

        # âœ… ì»¬ë ‰ì…˜ ì´ë¦„ì„ v3ë¡œ ì €ì¥ (UIì™€ ì¼ì¹˜)
        db.collection("generated_articles_v3").add(doc)
        created += 1
        print(f"Generated article for cluster {cluster_key}, total created={created}")

    log_event(db, "generate_done", {"created": created})
    print(f"Found {len(groups)} clusters, generated={created}")

if __name__ == "__main__":
    run_once()
