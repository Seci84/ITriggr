import os
import re
import json
import time
import traceback
from collections import defaultdict
from firebase_admin import firestore
from common import init_db, log_event, sim_prefix
from openai.types.chat.completion_create_params import ResponseFormat
from openai import OpenAI
from google.cloud.firestore_v1.base_query import FieldFilter
import requests
from bs4 import BeautifulSoup

# --- OpenAI ì‚¬ìš© ì—¬ë¶€ ---
client = None  # âœ… í•­ìƒ ë¯¸ë¦¬ ì •ì˜
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
USE_OPENAI = os.getenv("USE_OPENAI", "False").lower() == "true"

if OPENAI_API_KEY and USE_OPENAI:
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)  # proxies ì¸ì ì œê±°
        print("âœ… OpenAI client initialized successfully")
    except Exception as e:
        print(f"âŒ OpenAI client init failed: {e}")
        print("Full stack trace:")
        traceback.print_exc()
        USE_OPENAI = False
else:
    print(f"USE_OPENAI = {USE_OPENAI}, OPENAI_API_KEY is {'set' if OPENAI_API_KEY else 'not set'}")

# --- LLM í”„ë¡¬í”„íŠ¸: JSONë§Œ! (ì£¼ì„/ì½”ë“œíœìŠ¤ ê¸ˆì§€) ---
# UI êµ¬ì¡°ì— ë§ì¶° insights/actions ìŠ¤í‚¤ë§ˆ í™•ì¥ (general, entrepreneur, politician, investor)

PROMPT = """You are a news rewrite assistant. Return ONLY a single JSON object with no code fences, no explanations, and no comments.

Required JSON shape (all fields are MANDATORY and must match exactly):
{{
  "title": "string",
  "summary": "string",
  "bullets": ["string", "string", "string"],
  "facts": [{{"text":"string","evidence_url":"string"}}],
  "insights": {{
    "general": "string",
    "entrepreneur": "string",
    "politician": "string",
    "investor": "string"
  }},
  "actions": {{
    "general": [{{"action":"string","assumptions":"string","risk":"string","alternative":"string"}}],
    "entrepreneur": [{{"action":"string","assumptions":"string","risk":"string","alternative":"string"}}],
    "politician": [{{"action":"string","assumptions":"string","risk":"string","alternative":"string"}}],
    "investor": [{{"action":"string","assumptions":"string","risk":"string","alternative":"string"}}]
  }}
}}

Rules:
- Strictly adhere to the exact JSON shape above. Any deviation (e.g., comments, code blocks, explanations) will result in rejection.
- Detect the category (politics, economy, society, tech, military, etc.) from the content and tailor the analysis to it (e.g., tech: focus on innovations, military: strategic implications).
- Use available sources (one or more). Cite at least 1 item in "facts" with evidence_url chosen from the given Sources list. Be specific: name companies, products, or laws.
- Analyze the full content of each source URL to inform the title, summary, bullets, facts, insights, and actions. Provide multi-faceted information: e.g., market size, specific examples, related entities.
- Use a cautious, factual tone. No guarantees/advice. Use phrases like "possible idea" or "consider exploring".
- If mostly Korean sources, write Korean; otherwise English.
- For insights and actions, generate specific, concrete suggestions based on reader type:
  - General: Suggest skill learning for career opportunities (e.g., "Learn quantum computing via Coursera for roles like Astronautical Engineer at SpaceX") and small investments (e.g., "Specific US ETF: ARKX or UFO with SpaceX exposure").
  - Entrepreneur: Propose business opportunities like M&A or partnerships, naming specific companies (e.g., "Quantum Technologies for laser comm patents") and challenges (e.g., "Boeing's supply chain issues").
  - Politician: Recommend legislation or diplomacy (e.g., "Strengthen Space Policy Directives for quantum navigation; address gaps in international accords like Artemis").
  - Investor: Advise on stocks, chained opportunities, and troubled firms (e.g., "Invest in ARKX ETF for SpaceX exposure; ULA facing market share loss to SpaceX").

Sources:
{sources}
"""

def safe_parse_json(content: str):
    """LLM ì‘ë‹µì—ì„œ JSONë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ."""
    try:
        return json.loads(content)
    except Exception:
        pass
    content2 = re.sub(r"^```(?:json)?\s*|\s*```$", "", content.strip(), flags=re.I | re.M)
    try:
        return json.loads(content2)
    except Exception:
        pass
    m = re.search(r"\{.*\}", content, flags=re.S)
    if m:
        return json.loads(m.group(0))
    raise ValueError(f"JSON parse failed. head={content[:120]!r}")

def fetch_content(url):
    """URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        # ê°„ë‹¨í•œ ë³¸ë¬¸ ì¶”ì¶œ (ì‚¬ì´íŠ¸ë³„ë¡œ ì¡°ì • í•„ìš”)
        paragraphs = soup.find_all('p')
        content = ' '.join(p.get_text() for p in paragraphs if p.get_text().strip())
        return content[:1000]  # í† í° ì œí•œìœ¼ë¡œ 1000ì ì œí•œ
    except Exception as e:
        print(f"Failed to fetch content from {url}: {e}")
        return "Content unavailable"

from urllib.parse import urlparse

def load_recent_raw_groups(db, window_sec=6 * 60 * 60, prefix_bits=16,
                           exclude_domains=("nytimes.com", "nyti.ms")):
    """
    ìµœê·¼ ì›ë¬¸ ê¸°ì‚¬ë“¤ì„ simhash prefixë¡œ í´ëŸ¬ìŠ¤í„°ë§.
    NYT ë„ë©”ì¸(nytimes.com, nyti.ms)ì€ ì œì™¸.
    """
    def _is_excluded(url: str) -> bool:
        try:
            host = urlparse(url or "").netloc.lower()
            if not host:
                return False
            # example: sub.domain.nytimes.com ë„ í•¨ê»˜ ì œì™¸
            return any(host == d or host.endswith("." + d) for d in exclude_domains)
        except Exception:
            return False

    now = int(time.time())
    since = now - window_sec
    q = db.collection("raw_articles").where(filter=FieldFilter("published_at", ">=", since))
    groups = defaultdict(list)

    total, skipped = 0, 0
    for d in q.stream():
        it = d.to_dict() or {}
        total += 1
        url = it.get("url", "") or ""
        if _is_excluded(url):
            skipped += 1
            continue

        k = sim_prefix(it.get("simhash", ""), prefix_bits=prefix_bits)
        groups[k].append((d.id, it))

    print(f"Loaded {len(groups)} clusters from raw_articles (total_docs={total}, skipped_nyt={skipped})")
    return groups

def already_generated(db, cluster_key: str) -> bool:
    """
    íŠ¹ì • cluster_keyì— ëŒ€í•´ ì´ë¯¸ ìƒì„±ëœ ë¬¸ì„œê°€ ìˆëŠ”ì§€ í™•ì¸.
    (generated_articles_v3 ê¸°ì¤€)
    """
    try:
        snap = (db.collection("generated_articles_v3")
                  .where(filter=FieldFilter("cluster_key", "==", cluster_key))
                  .limit(1)
                  .get())
        return len(snap) > 0
    except Exception as e:
        print(f"already_generated check failed: {e}")
        return False


def make_payload_from_sources(items):
    """OPENAI ë¹„í™œì„±/ì‹¤íŒ¨ ì‹œ UIê°€ ë°”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” í…œí”Œë¦¿ í˜ì´ë¡œë“œ."""
    n = len(items)
    title = f"[Auto] {n} source{'s' if n > 1 else ''} on same event"
    summary = (
        "Multiple outlets reported a similar event. (Template summary: LLM disabled)"
        if n > 1
        else "A single source reported this event. (Template summary: LLM disabled)"
    )
    bullets = ["Key point 1", "Key point 2", "Key point 3"]
    first = items[0][1] if items else {}
    facts = [{"text": first.get("title", ""), "evidence_url": first.get("url", "")}]

    # âœ… UIê°€ ìš”êµ¬í•˜ëŠ” reader_typesì— ë§ì¶° ê¸°ë³¸ insights/actions ì±„ìš°ê¸°
    insights = {
        "general": "ì´ ì´ìŠˆëŠ” ì¼ìƒ ìƒí™œ/ì§ì¥ í™˜ê²½ì— ì ì§„ì  ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "entrepreneur": "ì‹œì¥ ê³µë°±ê³¼ ê·œì œ ë³€í™”ì—ì„œ ì´ˆê¸° ì§„ì… ê¸°íšŒë¥¼ íƒìƒ‰í•˜ì‹­ì‹œì˜¤.",
        "politician": "ë²•/ê·œì œ ì—…ë°ì´íŠ¸ í•„ìš”ì„±ê³¼ êµ­ì œ ê³µì¡° ì´ìŠˆë¥¼ ì ê²€í•˜ì‹­ì‹œì˜¤.",
        "investor": "í…Œë§ˆ/ì„¹í„° ë¦¬ìŠ¤í¬ì™€ í€ë”ë©˜í„¸ ê´´ë¦¬ë¥¼ êµ¬ë¶„í•´ ëª¨ë‹ˆí„°ë§í•˜ì‹­ì‹œì˜¤."
    }
    actions = {
        "general": [{
            "action": "ê³µì‹ ë³´ë„ìë£Œ ë° ì •ë¶€ ë°œí‘œë¥¼ íŒ”ë¡œì—…í•´ í•µì‹¬ ë³€ê²½ì‚¬í•­ íŒŒì•…",
            "assumptions": "ì •ì±…/ê¸°ì—… ë°œí‘œê°€ ì‹¤ì œ í–‰ë™ìœ¼ë¡œ ì´ì–´ì§ˆ ê°€ëŠ¥ì„±",
            "risk": "ì´ˆê¸° ë³´ë„ ê³¼ì¥ ë˜ëŠ” ì •ì • ë³´ë„",
            "alternative": "ì „ë¬¸ê°€ ë‰´ìŠ¤ë ˆí„° êµ¬ë…ìœ¼ë¡œ 2ì°¨ ê²€ì¦"
        }],
        "entrepreneur": [{
            "action": "ê³ ê° í˜ì¸í¬ì¸íŠ¸ ì¸í„°ë·° ë° ë¦° í”„ë¡œí† íƒ€ì… í…ŒìŠ¤íŠ¸",
            "assumptions": "ì´ìŠˆê°€ ì‹¤ìˆ˜ìš” ë¬¸ì œë¥¼ ìœ ë°œ/í™•ëŒ€",
            "risk": "ì‹œì¥ ìˆ˜ìš” ê³¼ëŒ€í‰ê°€",
            "alternative": "PoC/íŒŒì¼ëŸ¿ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ë¶„ì‚°"
        }],
        "politician": [{
            "action": "ì´í•´ê´€ê³„ì ê°„ë‹´íšŒ ê°œìµœ ë° ì˜í–¥í‰ê°€(ê·œì œ/ê³ ìš©/ì•ˆì „) ì°©ìˆ˜",
            "assumptions": "ì •ì±… ê°œì… ì—¬ì§€ ì¡´ì¬",
            "risk": "ì •ì¹˜ì  ë°˜ë°œ/ì˜ˆì‚° ì œì•½",
            "alternative": "ê¶Œê³ ì•ˆ/ê°€ì´ë“œë¼ì¸ë¶€í„° ë‹¨ê³„ì  ì¶”ì§„"
        }],
        "investor": [{
            "action": "ì„¹í„° ETFë¡œ í…Œë§ˆ ìµìŠ¤í¬ì € ì†Œê·œëª¨ ì‹¤í—˜",
            "assumptions": "ë‰´ìŠ¤ ëª¨ë©˜í…€ì´ ë‹¨ê¸° ê°€ê²© ë³€ë™ ìœ ë°œ",
            "risk": "ê±°ì‹œ ë³€ìˆ˜ì— ì˜í•œ ì—­í’",
            "alternative": "í˜„ê¸ˆë¹„ì¤‘/í—·ì§€ë¡œ ë³€ë™ì„± ê´€ë¦¬"
        }]
    }

    return {
        "title": title,
        "summary": summary,
        "bullets": bullets,
        "facts": facts,
        "insights": insights,
        "actions": actions
    }

def run_once():
    db = init_db()
    groups = load_recent_raw_groups(db)
    created = 0

    for cluster_key, items in groups.items():
        if len(items) < 1:
            continue
        if already_generated(db, cluster_key):
            print(f"Skipping cluster {cluster_key}: already generated")
            continue

        src_lines = []
        ts_min, ts_max = 10 ** 12, 0
        for _id, it in items:
            url = it.get("url", "")
            title = it.get("title", "")
            content = fetch_content(url)  # URLì—ì„œ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            src_lines.append(f"- {title} | {url} | {content}")
            ts = int(it.get("published_at", 0) or 0)
            ts_min, ts_max = min(ts_min, ts), max(ts_max, ts)

        payload = None  # ì´ˆê¸°í™”
        token_usage = {"prompt": 0, "completion": 0}
        latency_ms = 0
        model_used = "template"

        if USE_OPENAI and len(src_lines) >= 1:
            try:
                t0 = time.time()
                prompt = PROMPT.format(sources="\n".join(src_lines))

                print(f"Sending OpenAI request for cluster {cluster_key} with {len(src_lines)} sources")
                resp = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.2,
                    response_format={"type": "json_object"},
                )
                latency_ms = int((time.time() - t0) * 1000)

                try:
                    token_usage["prompt"] = getattr(resp.usage, "prompt_tokens", 0)
                    token_usage["completion"] = getattr(resp.usage, "completion_tokens", 0)
                except Exception:
                    pass

                content = getattr(resp.choices[0].message, "content", None)
                if content is None and isinstance(resp.choices[0].message, dict):
                    content = resp.choices[0].message.get("content", "")

                print("ğŸ” LLM RESPONSE START")
                print(content)
                print("ğŸ” LLM RESPONSE END")

                payload = safe_parse_json(content)
                model_used = "gpt-4o-mini"

            except Exception as e:
                print(f"OpenAI error for cluster {cluster_key}: {repr(e)}")
                print("Trace:\n", traceback.format_exc())
                log_event(db, "openai_error", {
                    "msg": str(e),
                    "raw_content": content if 'content' in locals() else "N/A",
                    "cluster_key": cluster_key
                })
                # GPT ì‹¤íŒ¨ ì‹œ í…œí”Œë¦¿ ì‚¬ìš©
                payload = make_payload_from_sources(items)

        else:
            # OpenAI ë¹„í™œì„±í™” ì‹œ í…œí”Œë¦¿ ì‚¬ìš©
            payload = make_payload_from_sources(items)

        if payload is None:
            payload = make_payload_from_sources(items)  # ì•ˆì „ë§

        # Firestore ë¬¸ì„œ êµ¬ì„± (UIê°€ ì½ëŠ” í•„ë“œ í¬í•¨)
        doc = {
            "cluster_key": cluster_key,
            "title": payload.get("title", ""),
            "summary": payload.get("summary", ""),
            "bullets": payload.get("bullets", []),
            "facts": payload.get("facts", []),
            # âœ… UIì—ì„œ ì‚¬ìš©í•˜ëŠ” í•„ë“œ ì¶”ê°€
            "insights": payload.get("insights", {
                "general": "", "entrepreneur": "", "politician": "", "investor": ""
            }),
            "actions": payload.get("actions", {
                "general": [], "entrepreneur": [], "politician": [], "investor": []
            }),
            "evidence_urls": [line.split("|")[1].strip() for line in src_lines if "|" in line],
            "raw_refs": [x[0] for x in items],
            "published_window": {"start": ts_min, "end": ts_max},
            "model": model_used,
            "token_usage": token_usage,
            "latency_ms": latency_ms,
            "created_at": firestore.SERVER_TIMESTAMP,
        }

        # âœ… ì»¬ë ‰ì…˜ ì´ë¦„ì„ v3ë¡œ ì €ì¥ (UIì™€ ì¼ì¹˜)
        db.collection("generated_articles_v3").add(doc)
        created += 1
        print(f"Generated article for cluster {cluster_key}, total created={created}")

    log_event(db, "generate_done", {"created": created})
    print(f"Found {len(groups)} clusters, generated={created}")

if __name__ == "__main__":
    run_once()
